# -*- coding: utf-8 -*-
"""Social_project_twibott22_Complexity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fL8B0Tc0k_VyUdAG2Wi3Ol9u0uU7hQj4
"""

# Connect to your Google Drive. cresci +kaggle
from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Social_media_project/

!ls

FINAL_CSV = "/content/drive/MyDrive/Copy_of_final_results_twibot22.csv"
FINAL_PREFIX     = "/content/drive/MyDrive/Social_media_project/data/final_results_batch"
BATCH_SIZE       = 1000000    # split every million rows
PROCESSED_CSV    = "/content/drive/MyDrive/Social_media_project/data/final_results_twibot22.csv"

!wc -l /content/drive/MyDrive/Copy_of_final_results_twibot22.csv

!pip install pandas textstat tqdm

import re

def automated_readability_index(text):
    """
    Language-agnostic ARI:
      4.71*(chars/words) + 0.5*(words/sentences) ‚Äì 21.43
    """
    # 1) split into words (alphanumeric runs)
    words = re.findall(r'\w+', text, flags=re.UNICODE)
    word_count = len(words)
    # 2) count characters within those words
    char_count = sum(len(w) for w in words)
    # 3) approximate sentences by punctuation
    sentences = re.split(r'[.!?]+', text)
    sent_count = sum(1 for s in sentences if s.strip())

    if word_count == 0 or sent_count == 0:
        return None

    return 4.71 * (char_count / word_count) \
         + 0.5  * (word_count / sent_count) \
         - 21.43



"""Returns the Flesch Reading Ease Score.
The following table can be helpful to assess the ease of readability in a document.
The table is an example of values. While the maximum score is 121.22, there is no limit on how low the score can be. A negative score is valid.

Score	Difficulty
90-100	Very Easy
80-89	Easy
70-79	Fairly Easy
60-69	Standard
50-59	Fairly Difficult
30-49	Difficult
0-29	Very Confusing
"""

# import spacy

# # 1) load the English model
# nlp = spacy.load("en_core_web_sm")

# def sentence_complexity(text):
#     """
#     Returns a numeric complexity score for a single sentence text.
#     Combines:
#       - avg token count
#       - number of subordinate clauses
#       - max parse‚Äêtree depth
#     """

#     doc = nlp(text)

#     # a) sentence length (in words, excluding punctuation)
#     word_tokens = [t for t in doc if not t.is_punct]
#     length = len(word_tokens)

#     # b) subordinate‚Äëclause count
#     # spaCy labels common clausal deps as advcl, ccomp, xcomp, acl, relcl
#     clause_count = sum(1 for t in doc if t.dep_ in ("advcl","ccomp","xcomp","acl","relcl"))

#     # c) parse‚Äëtree depth: longest head‚Äëchain
#     def token_depth(tok):
#         depth = 0
#         while tok.head != tok:
#             depth += 1
#             tok = tok.head
#         return depth

#     max_depth = max(token_depth(t) for t in doc)

#     # you could also compute lexical density:
#     # content_words = sum(1 for t in doc if t.pos_ in ("NOUN","VERB","ADJ","ADV"))
#     # lex_density = content_words / length if length else 0

#     # simple combined score (you can tweak the weights)
#     score = length + clause_count + max_depth
#     return score

import re

def detailed_sentence_complexity(text):
    """
    A more detailed, lightweight complexity metric that includes:
      - Average words per sentence
      - Count of subordinating conjunctions
      - Count of coordinating conjunctions (and, but, or)
      - Contribution of punctuation (commas, semicolons)
      - Length penalty for very short or very long sentences

    Parameters:
      text (str): The sentence or paragraph to score.

    Returns:
      float: A complexity score (higher indicates more complex).
    """
    # 1. Tokenize words
    words = re.findall(r'\w+', text)
    num_words = len(words)

    # 2. Estimate number of sentences
    sentences = re.split(r'[.!?]+', text)
    num_sentences = sum(1 for s in sentences if s.strip())
    if num_sentences == 0:
        num_sentences = 1

    avg_words = num_words / num_sentences

    # 3. Count subordinators
    subords = re.findall(r'\b(because|although|since|that|which)\b', text, flags=re.IGNORECASE)
    num_subords = len(subords)

    # 4. Count coordinators
    coords = re.findall(r'\b(and|but|or)\b', text, flags=re.IGNORECASE)
    num_coords = len(coords)

    # 5. Count punctuation complexity
    num_commas = text.count(',')
    num_semis = text.count(';')
    punct_complexity = num_commas * 0.5 + num_semis * 1.0

    # 6. Sentence length penalty/bonus
    length_penalty = 0
    if avg_words < 4:
        length_penalty = -1  # very short sentences simplify
    elif avg_words > 20:
        length_penalty = 1   # overly long sentences add complexity

    # 7. Combine with weights
    score = (
        0.4 * avg_words +
        0.3 * num_subords +
        0.1 * num_coords +
        0.1 * punct_complexity +
        0.1 * length_penalty
    )

    return score

import glob
import os
import pandas as pd

BOT_ONLY_FILES = {

    "bot_tweets.csv"
    # add any other filenames (or patterns) that you know are 100% bot
}
HUMAN_ONLY_FILES = {
    "tweet_8-005_humans.csv"

}

DATA_DIR ="/content/drive/MyDrive/Social_media_project/data/twibot22"
POSSIBLE_DATE_COLS = ["date","created_at","Created At","timestamp"]
POSSIBLE_BOT_COLS  = ["Bot Label"]
POSSIBLE_TEXT_COLS = ['Tweet', "description","text"]
csv_paths = glob.glob(os.path.join(DATA_DIR, "*.csv"))
print(csv_paths)

import textstat
import warnings
def compute_metrics(df, writer, counter, skip_count, master_df):
    """
    Streams metrics for each row of `df` into `writer` and appends to master_df.
    Prints every 100 rows, returns updated counter.
    """
    # detect columns
    text_col = next(c for c in POSSIBLE_TEXT_COLS if c in df.columns)
    date_col = next(c for c in POSSIBLE_DATE_COLS if c in df.columns)
    bot_col  = next((c for c in POSSIBLE_BOT_COLS if c in df.columns), None)

    for _, row in df.iterrows():
        counter += 1
        # skip previously processed rows
        if counter <= skip_count:
            continue
        # complexity
        txt        = str(row[text_col])
        complexity = detailed_sentence_complexity(txt)

        # year_month


        dt         = pd.to_datetime(row[date_col], errors="coerce")
        if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
           dt = dt.replace(tzinfo=None)
        year_month = dt.to_period("M").to_timestamp()

        # bot label
        if bot_col:
            bot_label = "human" if row[bot_col] == 0 else "bot"
        elif df.attrs.get("force_human"):
            bot_label = "human"
        else:
            bot_label = "bot"

        # write one line to CSV
        writer.writerow([year_month, complexity, bot_label])
        # append one row to DataFrame
        master_df.loc[len(master_df)] = [year_month, complexity, bot_label]

        # progress every 100 new rows
        new_rows = counter - skip_count
        if new_rows % 100 == 0:
            print(f"‚úÖ Processed {new_rows} NEW rows so far")

    return counter

!pip install chardet -q
import chardet
import pandas as pd

def load_csv_with_fallback(path, sample_size=100_000):
    """
    1) Try UTF‚Äë8 first.
    2) On failure, peek at the first `sample_size` bytes to guess the encoding.
    3) Re‚Äëopen with that encoding and errors="replace" so pandas only ever
       sees valid Unicode.
    """
    try:
        return pd.read_csv(path, encoding="utf-8")
    except UnicodeDecodeError:
        raw = open(path, "rb").read(sample_size)
        guess = chardet.detect(raw)
        enc   = guess["encoding"] or "latin-1"
        print(f"‚ö†Ô∏è  UTF‚Äë8 failed for {path!r}; retrying as {enc!r}")
        with open(path, "r", encoding=enc, errors="replace") as f:
            return pd.read_csv(f)

if os.path.exists(FINAL_CSV):
   # count lines, subtract 1 for header
    skip_count = sum(1 for _ in open(FINAL_CSV, "r")) - 1
    mode       = "a"
    write_hdr  = False
    print(f"üîÑ Resuming, skipping {skip_count} already-written rows")
else:
    skip_count = 0
    mode       = "w"
    write_hdr  = True
    # create file with header
    open(FINAL_CSV, "w", newline="").close()
    print(f"üÜï Starting fresh; will write header")

import os,csv
master_df = pd.DataFrame(columns=["year_month", "complexity", "bot"])

all_paths = glob.glob(os.path.join(DATA_DIR, "**", "*.csv"), recursive=True)
skip_count = 0
print(f"üîÑ Resuming, skipping {skip_count} already-written rows")


# 1) Path to write your rolling‚Äêcheckpoint & final file

# Open once in the right mode
# with open(FINAL_CSV, mode, newline="") as fout:
#     writer = csv.writer(fout)
#     if write_hdr:
#         writer.writerow(["year_month","complexity","bot"])
#     total = 0
with open(FINAL_CSV, "a", newline="") as fout:
    writer = csv.writer(fout)
    total = 0
# 6) Load, process, concat

    for path in csv_paths:
     print("‚ü≥ Loading", path)
    #df = pd.read_csv(path)
     df = load_csv_with_fallback(path)
    # If this is the problematic file, drop its created_at
     filename = os.path.basename(path)
     if filename == "bot_tweets.csv":
        print('skipping')
        continue
    # tell compute_metrics which ‚Äúflavor‚Äù this is
     if filename in BOT_ONLY_FILES:
        df.attrs["force_bot"]   = True
     if filename in HUMAN_ONLY_FILES:
        df.attrs["force_human"] = True

    #df.attrs["source_path"] = path
     total = compute_metrics(df, writer, total, skip_count, master_df)

print(f"üéâ Finished! Wrote {total} rows to {FINAL_CSV}")
print(master_df.head(10))

# if you‚Äôre in Colab and want to save to your Drive:

master = pd.read_csv(FINAL_CSV, parse_dates=["year_month"])

# 2) (Optional) Inspect the data
print(master["bot"].unique)

print("Unique months:", master["year_month"].dt.to_period("M").unique())

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# fig, ax = plt.subplots(figsize=(12, 6))
# sns.lineplot(
#     data=master,
#     x="year_month",
#     y="readability",
#     hue="bot",
#     marker="o",
#     ax=ax
# )
# ax.set_title("Avg. Readability by Month & Bot/Human")

# # Format the x-axis to show e.g. "2021-03"
# ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))   # tick every 2 months
# ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()

fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(
    data=master,
    x="year_month",
    y="complexity",
    hue="bot",
    marker="o",
    ax=ax
)
ax.set_title("Avg.Complexity by Month & Bot/Human")

# Format the x-axis to show e.g. "2021-03"
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))   # tick every 2 months
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""use ARI to handle that dataset has different language except english"""

