# -*- coding: utf-8 -*-
"""Social_project_llm_complexity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z83GZ9gUQ9oCj-jW1qCntT4gaCwETBcH
"""

# Connect to your Google Drive. cresci +kaggle
from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Social_media/data

!ls

FINAL_CSV = "/content/drive/MyDrive/Social_media/data/final_results_llm_complexity.csv"

!pip install pandas textstat tqdm

import re

def automated_readability_index(text):
    """
    Language-agnostic ARI:
      4.71*(chars/words) + 0.5*(words/sentences) ‚Äì 21.43
    """
    # 1) split into words (alphanumeric runs)
    words = re.findall(r'\w+', text, flags=re.UNICODE)
    word_count = len(words)
    # 2) count characters within those words
    char_count = sum(len(w) for w in words)
    # 3) approximate sentences by punctuation
    sentences = re.split(r'[.!?]+', text)
    sent_count = sum(1 for s in sentences if s.strip())

    if word_count == 0 or sent_count == 0:
        return None

    return 4.71 * (char_count / word_count) \
         + 0.5  * (word_count / sent_count) \
         - 21.43

# import spacy

# # 1) load the English model
# nlp = spacy.load("en_core_web_sm")

# def sentence_complexity(text):
#     """
#     Returns a numeric complexity score for a single sentence text.
#     Combines:
#       - avg token count
#       - number of subordinate clauses
#       - max parse‚Äêtree depth
#     """

#     doc = nlp(text)

#     # a) sentence length (in words, excluding punctuation)
#     word_tokens = [t for t in doc if not t.is_punct]
#     length = len(word_tokens)

#     # b) subordinate‚Äëclause count
#     # spaCy labels common clausal deps as advcl, ccomp, xcomp, acl, relcl
#     clause_count = sum(1 for t in doc if t.dep_ in ("advcl","ccomp","xcomp","acl","relcl"))

#     # c) parse‚Äëtree depth: longest head‚Äëchain
#     def token_depth(tok):
#         depth = 0
#         while tok.head != tok:
#             depth += 1
#             tok = tok.head
#         return depth

#     max_depth = max(token_depth(t) for t in doc)

#     # you could also compute lexical density:
#     # content_words = sum(1 for t in doc if t.pos_ in ("NOUN","VERB","ADJ","ADV"))
#     # lex_density = content_words / length if length else 0

#     # simple combined score (you can tweak the weights)
#     score = length + clause_count + max_depth
#     return score

# # Example usage
# for sent in [
#     "I went home.",
#     "Although it was raining, I decided to go for a long walk because I needed the fresh air."
# ]:
#     print(sent, "‚Üí", sentence_complexity(sent))

# import spacy

# # 1) load the English model
# nlp = spacy.load("en_core_web_sm")

# def sentence_complexity(text):
#     """
#     Returns a numeric complexity score for a single sentence text.
#     Combines:
#       - avg token count
#       - number of subordinate clauses
#       - max parse‚Äêtree depth
#     """

#     doc = nlp(text)

#     # a) sentence length (in words, excluding punctuation)
#     word_tokens = [t for t in doc if not t.is_punct]
#     length = len(word_tokens)

#     # b) subordinate‚Äëclause count
#     # spaCy labels common clausal deps as advcl, ccomp, xcomp, acl, relcl
#     clause_count = sum(1 for t in doc if t.dep_ in ("advcl","ccomp","xcomp","acl","relcl"))

#     # c) parse‚Äëtree depth: longest head‚Äëchain
#     def token_depth(tok):
#         depth = 0
#         while tok.head != tok:
#             depth += 1
#             tok = tok.head
#         return depth

#     max_depth = max(token_depth(t) for t in doc)

#     # you could also compute lexical density:
#     # content_words = sum(1 for t in doc if t.pos_ in ("NOUN","VERB","ADJ","ADV"))
#     # lex_density = content_words / length if length else 0

#     # simple combined score (you can tweak the weights)
#     score = length + clause_count + max_depth
#     return score

import re

def detailed_sentence_complexity(text):
    """
    A more detailed, lightweight complexity metric that includes:
      - Average words per sentence
      - Count of subordinating conjunctions
      - Count of coordinating conjunctions (and, but, or)
      - Contribution of punctuation (commas, semicolons)
      - Length penalty for very short or very long sentences

    Parameters:
      text (str): The sentence or paragraph to score.

    Returns:
      float: A complexity score (higher indicates more complex).
    """
    # 1. Tokenize words
    words = re.findall(r'\w+', text)
    num_words = len(words)

    # 2. Estimate number of sentences
    sentences = re.split(r'[.!?]+', text)
    num_sentences = sum(1 for s in sentences if s.strip())
    if num_sentences == 0:
        num_sentences = 1

    avg_words = num_words / num_sentences

    # 3. Count subordinators
    subords = re.findall(r'\b(because|although|since|that|which)\b', text, flags=re.IGNORECASE)
    num_subords = len(subords)

    # 4. Count coordinators
    coords = re.findall(r'\b(and|but|or)\b', text, flags=re.IGNORECASE)
    num_coords = len(coords)

    # 5. Count punctuation complexity
    num_commas = text.count(',')
    num_semis = text.count(';')
    punct_complexity = num_commas * 0.5 + num_semis * 1.0

    # 6. Sentence length penalty/bonus
    length_penalty = 0
    if avg_words < 4:
        length_penalty = -1  # very short sentences simplify
    elif avg_words > 20:
        length_penalty = 1   # overly long sentences add complexity

    # 7. Combine with weights
    score = (
        0.4 * avg_words +
        0.3 * num_subords +
        0.1 * num_coords +
        0.1 * punct_complexity +
        0.1 * length_penalty
    )

    return score



"""Returns the Flesch Reading Ease Score.
The following table can be helpful to assess the ease of readability in a document.
The table is an example of values. While the maximum score is 121.22, there is no limit on how low the score can be. A negative score is valid.

Score	Difficulty
90-100	Very Easy
80-89	Easy
70-79	Fairly Easy
60-69	Standard
50-59	Fairly Difficult
30-49	Difficult
0-29	Very Confusing
"""

import glob
import os
import pandas as pd

BOT_ONLY_FILES = {

    "cleaned_social_spambots_2.csv",
    "cleaned_social_spambots_3.csv",
    "traditional_spambots1.csv",
    "traditional_spambots2.csv",
    "traditional_spambots3.csv"
    # add any other filenames (or patterns) that you know are 100% bot
}
HUMAN_ONLY_FILES = {
    "cleaned_batch_1001_3000.csv",
    "cleaned_llm_tweets.csv"

}

DATA_DIR ="/content/drive/MyDrive/Social_media/data"
POSSIBLE_DATE_COLS = ["date","created_at","Created At","timestamp"]
POSSIBLE_BOT_COLS  = ["Bot Label"]
POSSIBLE_TEXT_COLS = ['Tweet', "description","text","humanized_tweet"]
csv_paths = glob.glob(os.path.join(DATA_DIR, "*.csv"))
print(csv_paths)

# import textstat
# def compute_metrics(df):


#     # 4.1) detect which column to use
#     text_col = next((c for c in POSSIBLE_TEXT_COLS if c in df.columns), None)
#     #df = df[df[text_col].apply(is_english)]
#     if text_col is None:
#         raise KeyError(
#             f"No text column found in {list(df.columns)}; "
#             f"expected one of {POSSIBLE_TEXT_COLS}"
#         )
#     #df = df[df[text_col].astype(str).apply(is_english)]

#     texts = df[text_col].astype(str)

#     # 4.2) apply your metrics
#     df["readability"] = texts.apply(textstat.flesch_reading_ease)
#     #df["complexity"]  = texts.apply(sentence_complexity)

#     bot_col = next((c for c in POSSIBLE_BOT_COLS if c in df.columns), None)
#     if bot_col:
#         # map the 0/1 column
#         df["bot"] = df[bot_col].map({0: "human", 1: "bot"})
#     elif df.attrs.get("force_bot"):
#         df["bot"] = "bot"
#     elif df.attrs.get("force_human"):
#         df["bot"] = "human"
#     else:
#         df["bot"] = "unknown"   # or drop these rows later



#    # -- year extraction
#     date_col = next((c for c in POSSIBLE_DATE_COLS if c in df.columns), None)
#     if not date_col:
#         raise KeyError(f"No date column in {df.columns}")
#     df["year"] = pd.to_datetime(df[date_col]).dt.year


#     # return df[["year","readability","complexity","bot"]]
#     return df[["year","readability","bot"]]


import warnings
def compute_metrics(df, writer, counter, skip_count, master_df):
    """
    Streams metrics for each row of `df` into `writer` and appends to master_df.
    Prints every 100 rows, returns updated counter.
    """
    # detect columns
    text_col = next(c for c in POSSIBLE_TEXT_COLS if c in df.columns)
    date_col = next(c for c in POSSIBLE_DATE_COLS if c in df.columns)
    bot_col  = next((c for c in POSSIBLE_BOT_COLS if c in df.columns), None)

    for _, row in df.iterrows():
        counter += 1
        # skip previously processed rows
        if counter <= skip_count:
            continue
        # complexity
        txt = str(row[text_col])
        #readability = automated_readability_index(txt)
        complexity = detailed_sentence_complexity(txt)

        # year_month


        dt         = pd.to_datetime(row[date_col], errors="coerce")
        if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
           dt = dt.replace(tzinfo=None)
        year_month = dt.to_period("M").to_timestamp()

        # bot label
        if bot_col:
            bot_label = "human" if row[bot_col] == 0 else "bot"
        elif df.attrs.get("force_human"):
            bot_label = "human"
        else:
            bot_label = "bot"

        # write one line to CSV
        writer.writerow([year_month, complexity, bot_label])
        #writer.writerow([year_month, readability, bot_label])
        # append one row to DataFrame
        #master_df.loc[len(master_df)] = [year_month, complexity, bot_label]
        #master_df.loc[len(master_df)] = [year_month, readability, bot_label]

        # progress every 100 new rows
        new_rows = counter - skip_count
        if new_rows % 100 == 0:
            print(f"‚úÖ Processed {new_rows} NEW rows so far")

    return counter

!pip install chardet -q
import chardet
import pandas as pd

def load_csv_with_fallback(path, sample_size=100_000):
    """
    1) Try UTF‚Äë8 first.
    2) On failure, peek at the first `sample_size` bytes to guess the encoding.
    3) Re‚Äëopen with that encoding and errors="replace" so pandas only ever
       sees valid Unicode.
    """
    try:
        return pd.read_csv(path, encoding="utf-8")
    except UnicodeDecodeError:
        raw = open(path, "rb").read(sample_size)
        guess = chardet.detect(raw)
        enc   = guess["encoding"] or "latin-1"
        print(f"‚ö†Ô∏è  UTF‚Äë8 failed for {path!r}; retrying as {enc!r}")
        with open(path, "r", encoding=enc, errors="replace") as f:
            return pd.read_csv(f)

if os.path.exists(FINAL_CSV):
   # count lines, subtract 1 for header
    skip_count = sum(1 for _ in open(FINAL_CSV, "r")) - 1
    mode       = "a"
    write_hdr  = False
    print(f"üîÑ Resuming, skipping {skip_count} already-written rows")
else:
    skip_count = 0
    mode       = "w"
    write_hdr  = True
    # create file with header
    open(FINAL_CSV, "w", newline="").close()
    print(f"üÜï Starting fresh; will write header")

# # 5) Gather all CSV paths
# csv_paths = glob.glob(os.path.join(DATA_DIR, "**", "*.csv"), recursive=True)

# # 6) Load, process, concat
# all_dfs = []
# for path in csv_paths:
#     print("‚ü≥ Loading", path)
#     #df = pd.read_csv(path)
#     df = load_csv_with_fallback(path)
#     # If this is the problematic file, drop its created_at
#     filename = os.path.basename(path)
#     if filename == "traditional_spambots1.csv":
#         if "created_at" in df.columns:
#             print(f"  ‚ö†Ô∏è Dropping 'created_at' from {filename}")
#             df = df.drop(columns=["created_at"])
#     # tell compute_metrics which ‚Äúflavor‚Äù this is
#     if filename in BOT_ONLY_FILES:
#         df.attrs["force_bot"]   = True
#     if filename in HUMAN_ONLY_FILES:
#         df.attrs["force_human"] = True

#     #df.attrs["source_path"] = path
#     metrics_df = compute_metrics(df)
#     all_dfs.append(compute_metrics(df))

# master = pd.concat(all_dfs, ignore_index=True)
# print(master.head(5))

import os,csv
master_df = pd.DataFrame(columns=["year_month", "complexity", "bot"])
#master_df = pd.DataFrame(columns=["year_month", "readability", "bot"])

all_paths = ['/content/drive/MyDrive/Social_media/data/cleaned_llm_tweets.csv',
              '/content/drive/MyDrive/Social_media/data/cleaned_batch_1001_3000.csv'
              ]


# 1) Path to write your rolling‚Äêcheckpoint & final file

# Open once in the right mode
with open(FINAL_CSV, mode, newline="") as fout:
    writer = csv.writer(fout)
    if write_hdr:
        writer.writerow(["year_month","complexity","bot"])
        #writer.writerow(["year_month","readability","bot"])
    total = 0
# 6) Load, process, concat

    for path in all_paths:
     print("‚ü≥ Loading", path)
    #df = pd.read_csv(path)
     df = load_csv_with_fallback(path)
    # If this is the problematic file, drop its created_at
     filename = os.path.basename(path)

    # tell compute_metrics which ‚Äúflavor‚Äù this is
     if filename in BOT_ONLY_FILES:
        df.attrs["force_bot"]   = True
     if filename in HUMAN_ONLY_FILES:
        df.attrs["force_human"] = True

    #df.attrs["source_path"] = path
     total = compute_metrics(df, writer, total, skip_count, master_df)

print(f"üéâ Finished! Wrote {total} rows to {FINAL_CSV}")
print(master_df.head(10))

master = pd.read_csv(FINAL_CSV, parse_dates=["year_month"])

# 2) (Optional) Inspect the data
print(master.head())
print("Unique months:", master["year_month"].dt.to_period("M").unique())

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(
    data=master,
    x="year_month",
    y="readability",
    hue="bot",
    marker="o",
    ax=ax
)
ax.set_title("Avg. Readability by Month & Bot/Human")

# Format the x-axis to show e.g. "2021-03"
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))   # tick every 2 months
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

unique_years = sorted(master["year"].unique())
print("Unique year values in datasets:", unique_years)