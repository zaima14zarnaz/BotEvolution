# -*- coding: utf-8 -*-
"""Social_project_twibott22_Readability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18BUzGmC6SlRdHrF5M40B0ZRZjfZ0DDBn
"""

# Connect to your Google Drive. twibot22
from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Social_media_project/

!ls

FINAL_CSV = "/content/drive/MyDrive/final_results_twibot22_readability.csv"

!pip install pandas textstat tqdm

import re

def automated_readability_index(text):
    """
    Language-agnostic ARI:
      4.71*(chars/words) + 0.5*(words/sentences) â€“ 21.43
    """
    # 1) split into words (alphanumeric runs)
    words = re.findall(r'\w+', text, flags=re.UNICODE)
    word_count = len(words)
    # 2) count characters within those words
    char_count = sum(len(w) for w in words)
    # 3) approximate sentences by punctuation
    sentences = re.split(r'[.!?]+', text)
    sent_count = sum(1 for s in sentences if s.strip())

    if word_count == 0 or sent_count == 0:
        return None

    return 4.71 * (char_count / word_count) \
         + 0.5  * (word_count / sent_count) \
         - 21.43



"""Returns the Flesch Reading Ease Score.
The following table can be helpful to assess the ease of readability in a document.
The table is an example of values. While the maximum score is 121.22, there is no limit on how low the score can be. A negative score is valid.

Score	Difficulty
90-100	Very Easy
80-89	Easy
70-79	Fairly Easy
60-69	Standard
50-59	Fairly Difficult
30-49	Difficult
0-29	Very Confusing
"""

# import spacy

# # 1) load the English model
# nlp = spacy.load("en_core_web_sm")

# def sentence_complexity(text):
#     """
#     Returns a numeric complexity score for a single sentence text.
#     Combines:
#       - avg token count
#       - number of subordinate clauses
#       - max parseâ€tree depth
#     """

#     doc = nlp(text)

#     # a) sentence length (in words, excluding punctuation)
#     word_tokens = [t for t in doc if not t.is_punct]
#     length = len(word_tokens)

#     # b) subordinateâ€‘clause count
#     # spaCy labels common clausal deps as advcl, ccomp, xcomp, acl, relcl
#     clause_count = sum(1 for t in doc if t.dep_ in ("advcl","ccomp","xcomp","acl","relcl"))

#     # c) parseâ€‘tree depth: longest headâ€‘chain
#     def token_depth(tok):
#         depth = 0
#         while tok.head != tok:
#             depth += 1
#             tok = tok.head
#         return depth

#     max_depth = max(token_depth(t) for t in doc)

#     # you could also compute lexical density:
#     # content_words = sum(1 for t in doc if t.pos_ in ("NOUN","VERB","ADJ","ADV"))
#     # lex_density = content_words / length if length else 0

#     # simple combined score (you can tweak the weights)
#     score = length + clause_count + max_depth
#     return score

import re

def detailed_sentence_complexity(text):
    """
    A more detailed, lightweight complexity metric that includes:
      - Average words per sentence
      - Count of subordinating conjunctions
      - Count of coordinating conjunctions (and, but, or)
      - Contribution of punctuation (commas, semicolons)
      - Length penalty for very short or very long sentences

    Parameters:
      text (str): The sentence or paragraph to score.

    Returns:
      float: A complexity score (higher indicates more complex).
    """
    # 1. Tokenize words
    words = re.findall(r'\w+', text)
    num_words = len(words)

    # 2. Estimate number of sentences
    sentences = re.split(r'[.!?]+', text)
    num_sentences = sum(1 for s in sentences if s.strip())
    if num_sentences == 0:
        num_sentences = 1

    avg_words = num_words / num_sentences

    # 3. Count subordinators
    subords = re.findall(r'\b(because|although|since|that|which)\b', text, flags=re.IGNORECASE)
    num_subords = len(subords)

    # 4. Count coordinators
    coords = re.findall(r'\b(and|but|or)\b', text, flags=re.IGNORECASE)
    num_coords = len(coords)

    # 5. Count punctuation complexity
    num_commas = text.count(',')
    num_semis = text.count(';')
    punct_complexity = num_commas * 0.5 + num_semis * 1.0

    # 6. Sentence length penalty/bonus
    length_penalty = 0
    if avg_words < 4:
        length_penalty = -1  # very short sentences simplify
    elif avg_words > 20:
        length_penalty = 1   # overly long sentences add complexity

    # 7. Combine with weights
    score = (
        0.4 * avg_words +
        0.3 * num_subords +
        0.1 * num_coords +
        0.1 * punct_complexity +
        0.1 * length_penalty
    )

    return score

import glob
import os
import pandas as pd

BOT_ONLY_FILES = {

    "bot_tweets.csv"
    # add any other filenames (or patterns) that you know are 100% bot
}
HUMAN_ONLY_FILES = { #change this file name for llm responses
    "tweet_8-005_humans.csv"

}
TARGET     = 1_000_000
DATA_DIR ="/content/drive/MyDrive/Social_media_project/data/twibot22"
POSSIBLE_DATE_COLS = ["date","created_at","Created At","timestamp"]
POSSIBLE_BOT_COLS  = ["Bot Label"]
POSSIBLE_TEXT_COLS = ['Tweet', "description","text","humanized_tweet"]
csv_paths = glob.glob(os.path.join(DATA_DIR, "*.csv"))
print(csv_paths)

import pandas as pd


import glob, os, math
import pandas as pd

pieces = []
TARGET     = 1_000_000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) First pass: count rows by year
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
year_counts = {}
paths = ['/content/drive/MyDrive/Social_media_project/data/twibot22/tweet_8-005_humans.csv']
for p in paths:
    # only read the date column
    df = pd.read_csv(p, usecols=['created_at'], parse_dates=['created_at'])
    yrs = df['created_at'].dt.year
    for y, cnt in yrs.value_counts().items():
        year_counts[y] = year_counts.get(y, 0) + cnt

# sort years by year value
years = sorted(year_counts)
n_years = len(years)
print("Found years:", years)

# 2) Compute base + remainder
base = TARGET // n_years
remainder = TARGET - base * n_years
print(f"Sampling {base} rows per year + 1 extra for the first {remainder} years")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) Second pass: build the subset
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


for idx, y in enumerate(years):
    # 1) Accumulate all rows for this year into a list of DataFrames
    frames = []
    for p in paths:
        # Only read the created_at column (faster!)
        tmp = pd.read_csv(p, usecols=["created_at"], parse_dates=["created_at"])
        # Filter to this year and keep only created_at
        df_year   = tmp[tmp["created_at"].dt.year == y]
        frames.append(df_year)

    # 2) Concatenate those into one DataFrame
    df_year = pd.concat(frames, ignore_index=True)

    # 3) Decide how many to sample
    want = base + (1 if idx < remainder else 0)
    avail = len(df_year)
    print(f" Year {y}: available={avail}, want={want}")

    if avail <= want:
        pieces.append(df_year)
    else:
        pieces.append(df_year.sample(n=want, random_state=42))

# 4) Glue all years back together
subset = pd.concat(pieces, ignore_index=True)

print(f"\nTotal rows in subset: {len(subset)}  (target was {TARGET})")
print("Per-year counts in subset:")
print(subset["created_at"].dt.year.value_counts().sort_index())



def compute_metrics(df, writer, counter, skip_count):
    """
    Streams metrics for each row of `df` into `writer` and appends to master_df.
    Prints every 100 rows, returns updated counter.
    """
    # detect columns
    text_col = next(c for c in POSSIBLE_TEXT_COLS if c in df.columns)
    date_col = next(c for c in POSSIBLE_DATE_COLS if c in df.columns)
    bot_col  = next((c for c in POSSIBLE_BOT_COLS if c in df.columns), None)

    for _, row in df.iterrows():
        counter += 1
        # skip previously processed rows
        if counter <= skip_count:
            continue
        # complexity
        txt        = str(row[text_col])
        #complexity = detailed_sentence_complexity(txt)
        readability = automated_readability_index(txt)

        # year_month


        dt         = pd.to_datetime(row[date_col], errors="coerce")
        if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
           dt = dt.replace(tzinfo=None)
        year_month = dt.to_period("M").to_timestamp()

        # bot label

        bot_label = "human"

        # write one line to CSV
        writer.writerow([year_month, readability, bot_label])
        # append one row to DataFrame
        #master_df.loc[len(master_df)] = [year_month, complexity, bot_label]
        #master_df.loc[len(master_df)] = [year_month, readability, bot_label]

        # progress every 100 new rows
        new_rows = counter - skip_count
        if new_rows % 100 == 0:
            print(f"âœ… Processed {new_rows} NEW rows so far")

    return counter

!pip install chardet -q
import chardet
import pandas as pd

def load_csv_with_fallback(path, sample_size=100_000):
    """
    1) Try UTFâ€‘8 first.
    2) On failure, peek at the first `sample_size` bytes to guess the encoding.
    3) Reâ€‘open with that encoding and errors="replace" so pandas only ever
       sees valid Unicode.
    """
    try:
        return pd.read_csv(path, encoding="utf-8")
    except UnicodeDecodeError:
        raw = open(path, "rb").read(sample_size)
        guess = chardet.detect(raw)
        enc   = guess["encoding"] or "latin-1"
        print(f"âš ï¸  UTFâ€‘8 failed for {path!r}; retrying as {enc!r}")
        with open(path, "r", encoding=enc, errors="replace") as f:
            return pd.read_csv(f)

if os.path.exists(FINAL_CSV):
   # count lines, subtract 1 for header
    skip_count = sum(1 for _ in open(FINAL_CSV, "r")) - 1
    mode       = "a"
    write_hdr  = False
    print(f"ğŸ”„ Resuming, skipping {skip_count} already-written rows")
else:
    skip_count = 0
    mode       = "w"
    write_hdr  = True
    # create file with header
    open(FINAL_CSV, "w", newline="").close()
    print(f"ğŸ†• Starting fresh; will write header")

import os,csv
master_df = pd.DataFrame(columns=["year_month", "readability", "bot"])

path = '/content/drive/MyDrive/Social_media_project/data/twibot22/tweet_8-005_humans.csv'
skip_count = 0
# print(f"ğŸ”„ Resuming, skipping {skip_count} already-written rows")


# 1) Path to write your rollingâ€checkpoint & final file

#Open once in the right mode
with open(FINAL_CSV, 'a', newline="") as fout:
    writer = csv.writer(fout)
    # if write_hdr:
    #     writer.writerow(["year_month","readability","bot"])
    total = 0
    raw_dfs = []
# 6) Load, process, concat
    print("âŸ³ Loading", path)
    #df = pd.read_csv(path)
    df = load_csv_with_fallback(path)
    # If this is the problematic file, drop its created_at
    filename = os.path.basename(path)
    raw_dfs.append(df)
    df_all = pd.concat(raw_dfs, ignore_index=True)
    df_all["year"] = pd.to_datetime(
    df_all[next(c for c in POSSIBLE_DATE_COLS if c in df_all.columns)],
    errors="coerce"
).dt.year

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) COUNT ROWS PER YEAR & DETERMINE STRATA SIZES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    year_counts = df_all["year"].value_counts().sort_index()
    years       = year_counts.index.tolist()
    n_years     = len(years)

    base      = TARGET // n_years
    remainder = TARGET - base * n_years
    print(f"Sampling {base} rows/year + 1 extra for first {remainder} years")

    pieces = []
    for idx, y in enumerate(years):
      df_y = df_all[df_all["year"] == y]
      want = base + (1 if idx < remainder else 0)
      avail = len(df_y)
      if avail <= want:
        print(f" Year {y}: only {avail}, taking all")
        pieces.append(df_y)
      else:
        print(f" Year {y}: sampling {want} of {avail}")
        pieces.append(df_y.sample(n=want, random_state=42))

    subset = pd.concat(pieces, ignore_index=True)
    print(f"â†’ Subset size: {len(subset)} rows")
    total = compute_metrics(subset, writer, total, skip_count)

print(f"ğŸ‰ Finished! Wrote {total} rows to {FINAL_CSV}")
print(master_df.head(10))

# if youâ€™re in Colab and want to save to your Drive:

import pandas as pd
FINAL_CSV1='/content/drive/MyDrive/final_results_twibot22_complexity_humanvsbot.csv'
master = pd.read_csv(FINAL_CSV1, parse_dates=["year_month"])

# 2) (Optional) Inspect the data
print(master["bot"].unique)

print("Unique months:", master["year_month"].dt.to_period("M").unique())

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(
    data=master,
    x="year_month",
    y="readability",
    hue="bot",
    marker="o",
    ax=ax
)
ax.set_title("Avg. Readability by Month & Bot/Human")

# # Format the x-axis to show e.g. "2021-03"
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))   # tick every 2 months
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# fig, ax = plt.subplots(figsize=(12, 6))
# sns.lineplot(
#     data=master,
#     x="year_month",
#     y="complexity",
#     hue="bot",
#     marker="o",
#     ax=ax
# )
# ax.set_title("Avg.Complexity by Month & Bot/Human")

# Format the x-axis to show e.g. "2021-03"
# ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))   # tick every 2 months
# ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()

"""use ARI to handle that dataset has different language except english"""

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
master = master[ master["year_month"] >= pd.Timestamp("2008-03-03") ]

# 2) Prepare a numeric date for trendâ€fitting
master["Year_Month"] = master["year_month"].map(mdates.date2num)

# 3) Base lineplot (no markers)
fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(
    data=master,
    x="year_month",
    y="complexity",
    hue="bot",
    marker=None,      # turn off dots
    ax=ax
)
ax.set_title("Avg. Complexity by Month (2008â€“) : Bot vs. Human")

# 4) Overlay one regression line per bot category
for bot_label, df_bot in master.groupby("bot"):
    sns.regplot(
        x="Year_Month",
        y="complexity",
        data=df_bot,
        scatter=False,        # no extra points
        ax=ax,
        label=f"{bot_label} trend",
        ci=None,
        line_kws={"linestyle":"-"}
    )

# 5) Tidy up the x-axis
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.xticks(rotation=45)

# Ensure our custom trend-labels show up in the legend
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles=handles, labels=labels, title="Source")

plt.tight_layout()
plt.show()


# # 1) Create a numeric version of year_month for regression
# master["serial_date"] = master["year_month"].map(mdates.date2num)

# # 2) Base line plot
# fig, ax = plt.subplots(figsize=(12, 6))
# sns.lineplot(
#     data=master,
#     x="year_month",
#     y="readability",
#     hue="bot",
#     marker="o",
#     ax=ax,
#     legend="brief"
# )

# # 3) Overlay one trend line per bot group
# for bot_label, df_bot in master.groupby("bot"):
#     sns.regplot(
#         x="serial_date",
#         y="readability",
#         data=df_bot,
#         scatter=False,
#         ax=ax,
#         label=f"{bot_label} trend",
#         ci=None,
#         line_kws={"linestyle":"--"}
#     )

# # 4) Tidy up the x-axis and legend
# ax.set_title("Avg. Readability by Month & Bot/Human (with Trendlines)")
# ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
# ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
# plt.xticks(rotation=45)
# ax.legend()
# plt.tight_layout()
# plt.show()

""" a higher ARI score indicates a more complex or difficult text, while a lower score suggests easier readability"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# â”€â”€â”€ 1. LOAD & FILTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MASTER_CSV = "/content/drive/MyDrive/final_results_twibot22_readability.csv"
date_col   = "year_month"
metric_col = "readability"
start_date = "2009-01-01"

# Load and keep only 2009 onwards
df = (
    pd.read_csv(MASTER_CSV, parse_dates=[date_col])
      .loc[lambda d: d[date_col] >= start_date]
      .set_index(date_col)
)

# â”€â”€â”€ 2. AGGREGATE INTO 6-MONTH BINS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
groups = df["bot"].unique()
aggs   = {}

for label in groups:
    sub = df[df["bot"] == label][[metric_col]]
    # resample into Jan/Jul bins
    agg = (
        sub
          .resample("6MS", origin="epoch")
          .agg(["mean", "count", "std"])
          .dropna()
    )
    mu    = agg[(metric_col, "mean")]
    n     = agg[(metric_col, "count")]
    sigma = agg[(metric_col, "std")]
    agg[(metric_col, "ci95")] = 1.96 * (sigma / np.sqrt(n))
    aggs[label] = agg

# align them so they share the exact same time index
all_idx = sorted(set().union(*(a.index for a in aggs.values())))
for label in aggs:
    aggs[label] = aggs[label].reindex(all_idx)

# â”€â”€â”€ 3. PLOT WITH SHADED CIs & TRENDLINES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
locator   = mdates.MonthLocator(bymonth=[1,7])  # ticks at Jan/Jul
formatter = mdates.DateFormatter("%Y-%m")
start, end = all_idx[0], all_idx[-1]

fig, ax = plt.subplots(figsize=(12, 6))
ymins, ymaxs = [], []

for label in groups:
    agg = aggs[label]
    mu = agg[(metric_col, "mean")]
    ci = agg[(metric_col, "ci95")]

    # 6-month mean line + shaded CI
    ax.plot(agg.index, mu, marker="o", linewidth=2, label=f"{label} 6-mo avg")
    ax.fill_between(agg.index, mu - ci, mu + ci, alpha=0.2)

    ymins.append((mu - ci).min())
    ymaxs.append((mu + ci).max())

    # dashed trendline
    valid = mu.dropna()
    if len(valid) > 1:
        x_vals = mdates.date2num(valid.index)
        trend = np.poly1d(np.polyfit(x_vals, valid.values, 1))
        ax.plot(
            all_idx,
            trend(mdates.date2num(pd.Index(all_idx))),
            linestyle="--",
            linewidth=2,
            label=f"{label} trend"
        )

# cosmetics
ax.set_xlim(start, end)
ax.set_ylim(2,12)
ax.xaxis.set_major_locator(locator)
ax.xaxis.set_major_formatter(formatter)
plt.xticks(rotation=45)

ax.set_title("6-Month Avg. Readability by Bot/Human (with 95 % CI & Trends)")
ax.set_xlabel("Date")
ax.set_ylabel("Avg Readability")
ax.legend(title="Series")
plt.tight_layout()
plt.show()