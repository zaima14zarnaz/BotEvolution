# -*- coding: utf-8 -*-
"""Social_project_llm_readability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KHO3CuVTcieCBt4FyPDBxqLCueP16Fz
"""

# Connect to your Google Drive. cresci +kaggle
from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Social_media_project/data/LLM/7 Batch 3001-4700

!ls

FINAL_CSV = "/content/drive/MyDrive/Social_media_project/data/LLM/COMBINED.csv"
FINAL_OUTPUT_CSV="/content/drive/MyDrive/final_results_llm_complexity.csv"

import os
import glob
import pandas as pd

# 1) Where your CSVs live
INPUT_DIR = "/content/drive/MyDrive/Social_media_project/data/LLM/7 Batch 3001-4700"


# 2) Find all CSV paths
csv_paths = glob.glob(os.path.join(INPUT_DIR, "*.csv"))

# 3) Load them all (with your fallback loader)
dfs = []
for p in csv_paths:
    print("Loading", os.path.basename(p))
    df = pd.read_csv(p)   # or pd.read_csv(p) if you don’t need fallback
    dfs.append(df)

# 4) Concatenate into one DataFrame
master = pd.concat(dfs, ignore_index=True, sort=False)

# 5) Write out
master.to_csv(FINAL_CSV, index=False)
print(f"✅ Wrote {len(master)} total rows to {FINAL_CSV}")

# import pandas as pd
# from tqdm import tqdm
# import textstat

# # Load dataset - traditional_spambots_3.csv
# #df = pd.read_csv('users.csv')

# #Load dataset - traditional_spambots_2.csv
# df = pd.read_csv('users_spambot2.csv')


# # Ensure Tweet column has no NaN
# df = df.dropna(subset=['description'])
# print(df.head(5))

df = pd.read_csv('data/cleaned_genuine_accounts.csv')
print(df.head(5))
unique_years = sorted(df["year"].unique())
print("Unique year values in datasets:", unique_years)

import re

def automated_readability_index(text):
    """
    Language-agnostic ARI:
      4.71*(chars/words) + 0.5*(words/sentences) – 21.43
    """
    # 1) split into words (alphanumeric runs)
    words = re.findall(r'\w+', text, flags=re.UNICODE)
    word_count = len(words)
    # 2) count characters within those words
    char_count = sum(len(w) for w in words)
    # 3) approximate sentences by punctuation
    sentences = re.split(r'[.!?]+', text)
    sent_count = sum(1 for s in sentences if s.strip())

    if word_count == 0 or sent_count == 0:
        return None

    return 4.71 * (char_count / word_count) \
         + 0.5  * (word_count / sent_count) \
         - 21.43

# import spacy

# # 1) load the English model
# nlp = spacy.load("en_core_web_sm")

# def sentence_complexity(text):
#     """
#     Returns a numeric complexity score for a single sentence text.
#     Combines:
#       - avg token count
#       - number of subordinate clauses
#       - max parse‐tree depth
#     """

#     doc = nlp(text)

#     # a) sentence length (in words, excluding punctuation)
#     word_tokens = [t for t in doc if not t.is_punct]
#     length = len(word_tokens)

#     # b) subordinate‑clause count
#     # spaCy labels common clausal deps as advcl, ccomp, xcomp, acl, relcl
#     clause_count = sum(1 for t in doc if t.dep_ in ("advcl","ccomp","xcomp","acl","relcl"))

#     # c) parse‑tree depth: longest head‑chain
#     def token_depth(tok):
#         depth = 0
#         while tok.head != tok:
#             depth += 1
#             tok = tok.head
#         return depth

#     max_depth = max(token_depth(t) for t in doc)

#     # you could also compute lexical density:
#     # content_words = sum(1 for t in doc if t.pos_ in ("NOUN","VERB","ADJ","ADV"))
#     # lex_density = content_words / length if length else 0

#     # simple combined score (you can tweak the weights)
#     score = length + clause_count + max_depth
#     return score

# # Example usage
# for sent in [
#     "I went home.",
#     "Although it was raining, I decided to go for a long walk because I needed the fresh air."
# ]:
#     print(sent, "→", sentence_complexity(sent))

# import spacy

# # 1) load the English model
# nlp = spacy.load("en_core_web_sm")

# def sentence_complexity(text):
#     """
#     Returns a numeric complexity score for a single sentence text.
#     Combines:
#       - avg token count
#       - number of subordinate clauses
#       - max parse‐tree depth
#     """

#     doc = nlp(text)

#     # a) sentence length (in words, excluding punctuation)
#     word_tokens = [t for t in doc if not t.is_punct]
#     length = len(word_tokens)

#     # b) subordinate‑clause count
#     # spaCy labels common clausal deps as advcl, ccomp, xcomp, acl, relcl
#     clause_count = sum(1 for t in doc if t.dep_ in ("advcl","ccomp","xcomp","acl","relcl"))

#     # c) parse‑tree depth: longest head‑chain
#     def token_depth(tok):
#         depth = 0
#         while tok.head != tok:
#             depth += 1
#             tok = tok.head
#         return depth

#     max_depth = max(token_depth(t) for t in doc)

#     # you could also compute lexical density:
#     # content_words = sum(1 for t in doc if t.pos_ in ("NOUN","VERB","ADJ","ADV"))
#     # lex_density = content_words / length if length else 0

#     # simple combined score (you can tweak the weights)
#     score = length + clause_count + max_depth
#     return score

import re

def detailed_sentence_complexity(text):
    """
    A more detailed, lightweight complexity metric that includes:
      - Average words per sentence
      - Count of subordinating conjunctions
      - Count of coordinating conjunctions (and, but, or)
      - Contribution of punctuation (commas, semicolons)
      - Length penalty for very short or very long sentences

    Parameters:
      text (str): The sentence or paragraph to score.

    Returns:
      float: A complexity score (higher indicates more complex).
    """
    # 1. Tokenize words
    words = re.findall(r'\w+', text)
    num_words = len(words)

    # 2. Estimate number of sentences
    sentences = re.split(r'[.!?]+', text)
    num_sentences = sum(1 for s in sentences if s.strip())
    if num_sentences == 0:
        num_sentences = 1

    avg_words = num_words / num_sentences

    # 3. Count subordinators
    subords = re.findall(r'\b(because|although|since|that|which)\b', text, flags=re.IGNORECASE)
    num_subords = len(subords)

    # 4. Count coordinators
    coords = re.findall(r'\b(and|but|or)\b', text, flags=re.IGNORECASE)
    num_coords = len(coords)

    # 5. Count punctuation complexity
    num_commas = text.count(',')
    num_semis = text.count(';')
    punct_complexity = num_commas * 0.5 + num_semis * 1.0

    # 6. Sentence length penalty/bonus
    length_penalty = 0
    if avg_words < 4:
        length_penalty = -1  # very short sentences simplify
    elif avg_words > 20:
        length_penalty = 1   # overly long sentences add complexity

    # 7. Combine with weights
    score = (
        0.4 * avg_words +
        0.3 * num_subords +
        0.1 * num_coords +
        0.1 * punct_complexity +
        0.1 * length_penalty
    )

    return score



"""Returns the Flesch Reading Ease Score.
The following table can be helpful to assess the ease of readability in a document.
The table is an example of values. While the maximum score is 121.22, there is no limit on how low the score can be. A negative score is valid.

Score	Difficulty
90-100	Very Easy
80-89	Easy
70-79	Fairly Easy
60-69	Standard
50-59	Fairly Difficult
30-49	Difficult
0-29	Very Confusing
"""

import glob
import os
import pandas as pd

BOT_ONLY_FILES = {

    "cleaned_social_spambots_2.csv",
    "cleaned_social_spambots_3.csv",
    "traditional_spambots1.csv",
    "traditional_spambots2.csv",
    "traditional_spambots3.csv"
    # add any other filenames (or patterns) that you know are 100% bot
}
HUMAN_ONLY_FILES = {
    "cleaned_genuine_accounts.csv"

}

DATA_DIR ="/content/drive/MyDrive/Social_media/data"
POSSIBLE_DATE_COLS = ["date","created_at","Created At","timestamp"]
POSSIBLE_BOT_COLS  = ["Bot Label"]
POSSIBLE_TEXT_COLS = ['Tweet', "description","text","humanized_tweet"]
csv_paths = glob.glob(os.path.join(DATA_DIR, "*.csv"))
print(csv_paths)

# import textstat
# def compute_metrics(df):


#     # 4.1) detect which column to use
#     text_col = next((c for c in POSSIBLE_TEXT_COLS if c in df.columns), None)
#     #df = df[df[text_col].apply(is_english)]
#     if text_col is None:
#         raise KeyError(
#             f"No text column found in {list(df.columns)}; "
#             f"expected one of {POSSIBLE_TEXT_COLS}"
#         )
#     #df = df[df[text_col].astype(str).apply(is_english)]

#     texts = df[text_col].astype(str)

#     # 4.2) apply your metrics
#     df["readability"] = texts.apply(textstat.flesch_reading_ease)
#     #df["complexity"]  = texts.apply(sentence_complexity)

#     bot_col = next((c for c in POSSIBLE_BOT_COLS if c in df.columns), None)
#     if bot_col:
#         # map the 0/1 column
#         df["bot"] = df[bot_col].map({0: "human", 1: "bot"})
#     elif df.attrs.get("force_bot"):
#         df["bot"] = "bot"
#     elif df.attrs.get("force_human"):
#         df["bot"] = "human"
#     else:
#         df["bot"] = "unknown"   # or drop these rows later



#    # -- year extraction
#     date_col = next((c for c in POSSIBLE_DATE_COLS if c in df.columns), None)
#     if not date_col:
#         raise KeyError(f"No date column in {df.columns}")
#     df["year"] = pd.to_datetime(df[date_col]).dt.year


#     # return df[["year","readability","complexity","bot"]]
#     return df[["year","readability","bot"]]


def compute_metrics(df, writer, counter, skip_count):
    """
    Streams metrics for each row of `df` into `writer` and appends to master_df.
    Prints every 100 rows, returns updated counter.
    """
    # detect columns
    text_col = next(c for c in POSSIBLE_TEXT_COLS if c in df.columns)
    date_col = next(c for c in POSSIBLE_DATE_COLS if c in df.columns)
    bot_col  = next((c for c in POSSIBLE_BOT_COLS if c in df.columns), None)

    for _, row in df.iterrows():
        counter += 1
        # skip previously processed rows
        if counter <= skip_count:
            continue
        # complexity
        txt = str(row[text_col])
        #readability = automated_readability_index(txt)
        complexity = detailed_sentence_complexity(txt)

        # year_month


        dt         = pd.to_datetime(row[date_col], errors="coerce")
        if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
           dt = dt.replace(tzinfo=None)
        year_month = dt.to_period("M").to_timestamp()

        # bot label

        bot_label = "human"

        # write one line to CSV
        writer.writerow([year_month, complexity, bot_label])
        #writer.writerow([year_month, readability, bot_label])
        # append one row to DataFrame


        # progress every 100 new rows
        new_rows = counter - skip_count
        if new_rows % 100 == 0:
            print(f"✅ Processed {new_rows} NEW rows so far")

    return counter

!pip install chardet -q
import chardet
import pandas as pd

def load_csv_with_fallback(path, sample_size=100_000):
    """
    1) Try UTF‑8 first.
    2) On failure, peek at the first `sample_size` bytes to guess the encoding.
    3) Re‑open with that encoding and errors="replace" so pandas only ever
       sees valid Unicode.
    """
    try:
        return pd.read_csv(path, encoding="utf-8")
    except UnicodeDecodeError:
        raw = open(path, "rb").read(sample_size)
        guess = chardet.detect(raw)
        enc   = guess["encoding"] or "latin-1"
        print(f"⚠️  UTF‑8 failed for {path!r}; retrying as {enc!r}")
        with open(path, "r", encoding=enc, errors="replace") as f:
            return pd.read_csv(f)

if os.path.exists(FINAL_CSV):
   # count lines, subtract 1 for header
    skip_count = sum(1 for _ in open(FINAL_CSV, "r")) - 1
    mode       = "a"
    write_hdr  = False
    print(f"🔄 Resuming, skipping {skip_count} already-written rows")
else:
    skip_count = 0
    mode       = "w"
    write_hdr  = True
    # create file with header
    open(FINAL_CSV, "w", newline="").close()
    print(f"🆕 Starting fresh; will write header")

# # 5) Gather all CSV paths
# csv_paths = glob.glob(os.path.join(DATA_DIR, "**", "*.csv"), recursive=True)

# # 6) Load, process, concat
# all_dfs = []
# for path in csv_paths:
#     print("⟳ Loading", path)
#     #df = pd.read_csv(path)
#     df = load_csv_with_fallback(path)
#     # If this is the problematic file, drop its created_at
#     filename = os.path.basename(path)
#     if filename == "traditional_spambots1.csv":
#         if "created_at" in df.columns:
#             print(f"  ⚠️ Dropping 'created_at' from {filename}")
#             df = df.drop(columns=["created_at"])
#     # tell compute_metrics which “flavor” this is
#     if filename in BOT_ONLY_FILES:
#         df.attrs["force_bot"]   = True
#     if filename in HUMAN_ONLY_FILES:
#         df.attrs["force_human"] = True

#     #df.attrs["source_path"] = path
#     metrics_df = compute_metrics(df)
#     all_dfs.append(compute_metrics(df))

# master = pd.concat(all_dfs, ignore_index=True)
# print(master.head(5))

import os,csv

# 1) Path to write your rolling‐checkpoint & final file

# Open once in the right mode
with open(FINAL_OUTPUT_CSV, 'a', newline="") as fout:
    writer = csv.writer(fout)
    skip_count = 0
    total = 0
# 6) Load, process, concat

    path=FINAL_CSV
    print("⟳ Loading", path)
    #df = pd.read_csv(path)
    df = load_csv_with_fallback(path)
    # If this is the problematic file, drop its created_at
    #df.attrs["source_path"] = path
    total = compute_metrics(df, writer, total, skip_count)

print(f"🎉 Finished! Wrote {total} rows to {FINAL_CSV}")

master = pd.read_csv(FINAL_CSV, parse_dates=["year_month"])

# 2) (Optional) Inspect the data
print(master.head())
print("Unique months:", master["year_month"].dt.to_period("M").unique())

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(
    data=master,
    x="year_month",
    y="readability",
    hue="bot",
    marker="o",
    ax=ax
)
ax.set_title("Avg. Readability by Month & Bot/Human")

# Format the x-axis to show e.g. "2021-03"
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))   # tick every 2 months
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# 1) Load the first CSV (has bot_level with 'bot' and 'human')
df1 = pd.read_csv(
    "/content/drive/MyDrive/final_results_twibot22_complexity_humanvsbot.csv",
    parse_dates=["year_month"]
)

# keep only the 'bot' rows
df1 = df1[df1["bot"] == "bot"]

# 2) Load the second CSV (has bot_label but only 'human' values)
df2 = pd.read_csv(
    "/content/drive/MyDrive/final_results_llm_complexity.csv",
    parse_dates=["year_month"]
)

df2["bot"] = df2["bot"].replace({"human": "llm"})

# 3) Combine them
master = pd.concat([df1, df2], ignore_index=True)

# 4) Plot
# fig, ax = plt.subplots(figsize=(12, 6))
# sns.lineplot(
#     data=master,
#     x="year_month",
#     y="complexity",     # or "complexity"
#     hue="bot",           # will now show two lines: 'bot' and 'llm'
#     marker="o",
#     ax=ax
# )
# ax.set_title("Avg. Complexity by Month: Bot vs. LLM")

# # format x-axis
# ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
# ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()


# — assume df1 and df2 are loaded & concatenated into master as you already did —

# 1) Filter to 2008 onward
master = master[ master["year_month"] >= pd.Timestamp("2008-03-03") ]

# 2) Prepare a numeric date for trend‐fitting
master["Year_Month"] = master["year_month"].map(mdates.date2num)

# 3) Base lineplot (no markers)
fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(
    data=master,
    x="year_month",
    y="complexity",
    hue="bot",
    marker=None,      # turn off dots
    ax=ax
)
ax.set_title("Avg. Complexity by Month (2008–) : Bot vs. LLM")

# 4) Overlay one regression line per bot category
for bot_label, df_bot in master.groupby("bot"):
    sns.regplot(
        x="Year_Month",
        y="complexity",
        data=df_bot,
        scatter=False,        # no extra points
        ax=ax,
        label=f"{bot_label} trend",
        ci=None,
        line_kws={"linestyle":"-"}
    )

# 5) Tidy up the x-axis
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.xticks(rotation=45)

# Ensure our custom trend-labels show up in the legend
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles=handles, labels=labels, title="Source")

plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# 1) Load the first CSV (has bot_level with 'bot' and 'human')
df1 = pd.read_csv(
    "/content/drive/MyDrive/final_results_twibot22_readability.csv",
    parse_dates=["year_month"]
)

# keep only the 'bot' rows
df1 = df1[df1["bot"] == "bot"]

# 2) Load the second CSV (has bot_label but only 'human' values)
df2 = pd.read_csv(
    "/content/drive/MyDrive/final_results_llm_readability.csv",
    parse_dates=["year_month"]
)

df2["bot"] = df2["bot"].replace({"bot": "llm"})

# 3) (Optional) verify

df2["bot"] = df2["bot"].replace({"human": "llm"})
# 3) (Optional) verify
print(df2["bot"].unique())
# 3) Combine them
master = pd.concat([df1, df2], ignore_index=True)

# 4) Plot
# fig, ax = plt.subplots(figsize=(12, 6))
# sns.lineplot(
#     data=master,
#     x="year_month",
#     y="readability",     # or "complexity"
#     hue="bot",           # will now show two lines: 'bot' and 'llm'
#     marker="o",
#     ax=ax
# )
# ax.set_title("Avg.Readability by Month: Bot vs. LLM")

# # format x-axis
# ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
# ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()
master = master[ master["year_month"] >= pd.Timestamp("2008-03-03") ]

# 2) Prepare a numeric date for trend‐fitting
master["Year_Month"] = master["year_month"].map(mdates.date2num)

# 3) Base lineplot (no markers)
fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(
    data=master,
    x="year_month",
    y="readability",
    hue="bot",
    marker=None,      # turn off dots
    ax=ax
)
ax.set_title("Avg. Readability by Month (2008–) : Bot vs. LLM")

# 4) Overlay one regression line per bot category
for bot_label, df_bot in master.groupby("bot"):
    sns.regplot(
        x="Year_Month",
        y="readability",
        data=df_bot,
        scatter=False,        # no extra points
        ax=ax,
        label=f"{bot_label} trend",
        ci=None,
        line_kws={"linestyle":"-"}
    )

# 5) Tidy up the x-axis
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.xticks(rotation=45)

# Ensure our custom trend-labels show up in the legend
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles=handles, labels=labels, title="Source")

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import os

# ─── 1.  FILE PATHS & LABELS ───────────────────────────────────────────────────
csv_llm = "/content/drive/MyDrive/final_results_llm_complexity.csv"
csv_bot = "/content/drive/MyDrive/final_results_twibot22.csv"
datasets = [
    (csv_llm, "LLM"),   # your fine-tuned LLM results
    (csv_bot, "Bot")    # original bot results
]

# ─── 2.  SHARED SETTINGS ───────────────────────────────────────────────────────
metric_col = "complexity"
date_col   = "year_month"     # timestamp column
start_date = "2009-01-01"

def load_and_aggregate(path, label):
    """
    1) Load CSV, parse dates, filter from start_date
    2) Resample into 6-month bins (Jan & Jul), compute mean/count/std
    3) Compute 95% CI
    """
    df = pd.read_csv(path, parse_dates=[date_col])
    df = df[df[date_col] >= start_date].set_index(date_col)

    # 6-month windows starting Jan/Jul
    agg = (
        df[[metric_col]]
          .resample("6MS", origin="epoch")
          .agg(["mean", "count", "std"])
          .dropna()
    )

    # CI = 1.96 * σ/√n
    mu    = agg[(metric_col, "mean")]
    n     = agg[(metric_col, "count")]
    sigma = agg[(metric_col, "std")]
    agg[(metric_col, "ci95")] = 1.96 * (sigma / np.sqrt(n))

    # give the DataFrame a name so we can reindex later
    agg.name = label
    return agg

# aggregate each
aggs = {label: load_and_aggregate(path, label) for path, label in datasets}

# align all indices so x-axis is shared
all_idx = sorted(set().union(*(df.index for df in aggs.values())))
for label in aggs:
    aggs[label] = aggs[label].reindex(all_idx)

# ─── 3.  PLOTTING ───────────────────────────────────────────────────────────────
locator   = mdates.MonthLocator(bymonth=[1, 7])  # Jan & Jul
formatter = mdates.DateFormatter("%Y‐%m")
start, end = all_idx[0], all_idx[-1]

for label in aggs:
    df = aggs[label]
    print(f"{label}: {len(df)} bins from {start.date()} to {end.date()}")

fig, ax = plt.subplots(figsize=(12, 6))
ymins, ymaxs = [], []
for label in datasets:
    lab = label[1]
    agg = aggs[lab]
    mu   = agg[(metric_col, "mean")]
    ci   = agg[(metric_col, "ci95")]

    # 6-month line + shaded CI
    ax.plot(agg.index, mu, marker="o", linewidth=2, label=f"{lab} 6-mo avg")
    ax.fill_between(agg.index, mu - ci, mu + ci, alpha=0.2)

    ymins.append((mu - ci).min())
    ymaxs.append((mu + ci).max())

    # dashed linear trend
    valid = mu.dropna()
    if len(valid) > 1:
        x = mdates.date2num(valid.index)
        trend = np.poly1d(np.polyfit(x, valid.values, 1))
        ax.plot(
            all_idx,
            trend(mdates.date2num(pd.Index(all_idx))),
            linestyle="--",
            linewidth=2,
            label=f"{lab} trend"
        )

# cosmetics
ax.set_xlim(start, end)
ax.set_ylim(2,6)
ax.xaxis.set_major_locator(locator)
ax.xaxis.set_major_formatter(formatter)
plt.xticks(rotation=45)
ax.set_title("6-Month Avg. Complexity with 95 % CI & Trendlines")
ax.set_xlabel("Date")
ax.set_ylabel("Avg Complexity")
ax.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import os

# ─── 1.  FILE PATHS & LABELS ───────────────────────────────────────────────────
csv_llm = "/content/drive/MyDrive/final_results_llm_readability.csv"
csv_bot = "/content/drive/MyDrive/final_results_twibot22_readability.csv"
datasets = [
    (csv_llm, "LLM"),   # your fine-tuned LLM results
    (csv_bot, "Bot")    # original bot results
]

# ─── 2.  SHARED SETTINGS ───────────────────────────────────────────────────────
metric_col = "readability"
date_col   = "year_month"     # timestamp column
start_date = "2009-01-01"

def load_and_aggregate(path, label):
    """
    1) Load CSV, parse dates, filter from start_date
    2) Resample into 6-month bins (Jan & Jul), compute mean/count/std
    3) Compute 95% CI
    """
    df = pd.read_csv(path, parse_dates=[date_col])
    df = df[df[date_col] >= start_date].set_index(date_col)

    # 6-month windows starting Jan/Jul
    agg = (
        df[[metric_col]]
          .resample("6MS", origin="epoch")
          .agg(["mean", "count", "std"])
          .dropna()
    )

    # CI = 1.96 * σ/√n
    mu    = agg[(metric_col, "mean")]
    n     = agg[(metric_col, "count")]
    sigma = agg[(metric_col, "std")]
    agg[(metric_col, "ci95")] = 1.96 * (sigma / np.sqrt(n))

    # give the DataFrame a name so we can reindex later
    agg.name = label
    return agg

# aggregate each
aggs = {label: load_and_aggregate(path, label) for path, label in datasets}

# align all indices so x-axis is shared
all_idx = sorted(set().union(*(df.index for df in aggs.values())))
for label in aggs:
    aggs[label] = aggs[label].reindex(all_idx)

# ─── 3.  PLOTTING ───────────────────────────────────────────────────────────────
locator   = mdates.MonthLocator(bymonth=[1, 7])  # Jan & Jul
formatter = mdates.DateFormatter("%Y‐%m")
start, end = all_idx[0], all_idx[-1]

for label in aggs:
    df = aggs[label]
    print(f"{label}: {len(df)} bins from {start.date()} to {end.date()}")

fig, ax = plt.subplots(figsize=(12, 6))
ymins, ymaxs = [], []
for label in datasets:
    lab = label[1]
    agg = aggs[lab]
    mu   = agg[(metric_col, "mean")]
    ci   = agg[(metric_col, "ci95")]

    # 6-month line + shaded CI
    ax.plot(agg.index, mu, marker="o", linewidth=2, label=f"{lab} 6-mo avg")
    ax.fill_between(agg.index, mu - ci, mu + ci, alpha=0.2)

    ymins.append((mu - ci).min())
    ymaxs.append((mu + ci).max())

    # dashed linear trend
    valid = mu.dropna()
    if len(valid) > 1:
        x = mdates.date2num(valid.index)
        trend = np.poly1d(np.polyfit(x, valid.values, 1))
        ax.plot(
            all_idx,
            trend(mdates.date2num(pd.Index(all_idx))),
            linestyle="--",
            linewidth=2,
            label=f"{lab} trend"
        )

# cosmetics
ax.set_xlim(start, end)
ax.set_ylim(2,12)
ax.xaxis.set_major_locator(locator)
ax.xaxis.set_major_formatter(formatter)
plt.xticks(rotation=45)
ax.set_title("6-Month Avg. Readability with 95 % CI & Trendlines")
ax.set_xlabel("Date")
ax.set_ylabel("Avg Readability")
ax.legend()
plt.tight_layout()
plt.show()